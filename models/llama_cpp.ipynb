{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa33MCCym7Sf0UtgrJBMsX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ixd-ai-hub/Research-Ground/blob/project%2FCU-865d7n88b-chatbot-creator/models/llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GChZkbFVygZ5",
        "outputId": "3cebe95c-7071-4949-f6a8-646febc7af31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 1844, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 1844 (delta 37), reused 53 (delta 21), pack-reused 1763\u001b[K\n",
            "Receiving objects: 100% (1844/1844), 2.03 MiB | 12.93 MiB/s, done.\n",
            "Resolving deltas: 100% (1153/1153), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd llama.cpp\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLEOnOIzyyFg",
        "outputId": "ce1b3ba1-8898-4157-a484-e2d381626529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n",
            "build.zig\t\t flake.lock  llama.h\t    prompts\n",
            "CMakeLists.txt\t\t flake.nix   llama_util.h   README.md\n",
            "convert-lora-to-ggml.py  ggml.c      Makefile\t    requirements.txt\n",
            "convert-pth-to-ggml.py\t ggml.h      media\t    SHA256SUMS\n",
            "convert.py\t\t LICENSE     models\t    spm-headers\n",
            "examples\t\t llama.cpp   Package.swift  tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Vk-DSPZ04i1",
        "outputId": "34a6bd29-1cda-408a-b6c6-73cd261d7db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build\t\t\t flake.lock  llama_util.h   quantize-stats\n",
            "build.zig\t\t flake.nix   main\t    README.md\n",
            "CMakeLists.txt\t\t ggml.c      Makefile\t    requirements.txt\n",
            "common.o\t\t ggml.h      media\t    SHA256SUMS\n",
            "convert-lora-to-ggml.py  ggml.o      models\t    spm-headers\n",
            "convert-pth-to-ggml.py\t LICENSE     Package.swift  tests\n",
            "convert.py\t\t llama.cpp   perplexity\n",
            "embedding\t\t llama.h     prompts\n",
            "examples\t\t llama.o     quantize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --header=\"Host: cdn-lfs.huggingface.co\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://huggingface.co/\" --header=\"Cookie: __stripe_mid=d4c32d74-9e96-4099-b172-a457d6acc6d161b90a; __stripe_sid=b9b65e1b-0304-496f-88a6-9ef9a4cfd5e85e038a\" --header=\"Connection: keep-alive\" \"https://cdn-lfs.huggingface.co/repos/5d/83/5d839c6bc8347a438505b9deeff167e9395feeeeac8f0f4157236bd6bfa66b42/4e398f05842206e08cdc5e7bb4f6c7c34b9dc373435ece6f261b14b7b4fe9b89?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-q4_0.bin%3B+filename%3D%22ggml-model-q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1682053606&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzVkLzgzLzVkODM5YzZiYzgzNDdhNDM4NTA1YjlkZWVmZjE2N2U5Mzk1ZmVlZWVhYzhmMGY0MTU3MjM2YmQ2YmZhNjZiNDIvNGUzOThmMDU4NDIyMDZlMDhjZGM1ZTdiYjRmNmM3YzM0YjlkYzM3MzQzNWVjZTZmMjYxYjE0YjdiNGZlOWI4OT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODIwNTM2MDZ9fX1dfQ__&Signature=DCtrBiYG66SbwYm91Q8ZsYMctsCQnX48tpiYM0M72ApZ1FsKE7%7EgHerHFw1-0LFnuzkWOUnhL1CvZQRcGlEbpzBQDXwkdJOe8JkXVsPD5PYZBRGHccOfmBxu3JXE4RhNNLOZuT5axYfbF9cdXBxjCBPVih4ox3qUIOPkn51ZeFdknTymlXk5Uo62HJuXzGLP%7EFC3QBFd8U8nnHfIqHNZ3TJe2Pr-%7EIi-nQCpRTIuWbPgo6B2ALCKPK%7E7xLNclPij3XEIni3dnUcBEvEeqeBSyK8iffaHInrudy3x%7En5lf%7EiP95coy3nGlBargqBVAA6Bwgw0LVkQwWxWZPkpV4jyUA__&Key-Pair-Id=KVTP0A1DKRTAX\" -c -O './models/7B/ggml-model-q4_0.bin'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf9Roc2y1P7T",
        "outputId": "a2d2641f-2b28-421c-d327-ce943a76f341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-18 05:28:00--  https://cdn-lfs.huggingface.co/repos/5d/83/5d839c6bc8347a438505b9deeff167e9395feeeeac8f0f4157236bd6bfa66b42/4e398f05842206e08cdc5e7bb4f6c7c34b9dc373435ece6f261b14b7b4fe9b89?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-q4_0.bin%3B+filename%3D%22ggml-model-q4_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1682053606&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzVkLzgzLzVkODM5YzZiYzgzNDdhNDM4NTA1YjlkZWVmZjE2N2U5Mzk1ZmVlZWVhYzhmMGY0MTU3MjM2YmQ2YmZhNjZiNDIvNGUzOThmMDU4NDIyMDZlMDhjZGM1ZTdiYjRmNmM3YzM0YjlkYzM3MzQzNWVjZTZmMjYxYjE0YjdiNGZlOWI4OT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODIwNTM2MDZ9fX1dfQ__&Signature=DCtrBiYG66SbwYm91Q8ZsYMctsCQnX48tpiYM0M72ApZ1FsKE7%7EgHerHFw1-0LFnuzkWOUnhL1CvZQRcGlEbpzBQDXwkdJOe8JkXVsPD5PYZBRGHccOfmBxu3JXE4RhNNLOZuT5axYfbF9cdXBxjCBPVih4ox3qUIOPkn51ZeFdknTymlXk5Uo62HJuXzGLP%7EFC3QBFd8U8nnHfIqHNZ3TJe2Pr-%7EIi-nQCpRTIuWbPgo6B2ALCKPK%7E7xLNclPij3XEIni3dnUcBEvEeqeBSyK8iffaHInrudy3x%7En5lf%7EiP95coy3nGlBargqBVAA6Bwgw0LVkQwWxWZPkpV4jyUA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.84.18.121, 52.84.18.124, 52.84.18.77, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.84.18.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5105954665 (4.8G) [application/octet-stream]\n",
            "Saving to: ‘./models/7B/ggml-model-q4_0.bin’\n",
            "\n",
            "./models/7B/ggml-mo 100%[===================>]   4.75G  71.9MB/s    in 63s     \n",
            "\n",
            "2023-04-18 05:29:03 (77.5 MB/s) - ‘./models/7B/ggml-model-q4_0.bin’ saved [5105954665/5105954665]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make -j && ./main -m ./models/7B/ggml-model-q4_0.bin -p \"Building a website can be done in 10 simple steps:\" -n 512"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dFaAVA40uRu",
        "outputId": "45233dcc-e88d-4279-b119-64417c8d20cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I llama.cpp build info: \n",
            "I UNAME_S:  Linux\n",
            "I UNAME_P:  x86_64\n",
            "I UNAME_M:  x86_64\n",
            "I CFLAGS:   -I.              -O3 -DNDEBUG -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wno-unused-function -pthread -march=native -mtune=native\n",
            "I CXXFLAGS: -I. -I./examples -O3 -DNDEBUG -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native\n",
            "I LDFLAGS:  \n",
            "I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
            "\n",
            "make: Nothing to be done for 'default'.\n",
            "main: seed = 1681795751\n",
            "llama.cpp: loading model from ./models/7B/ggml-model-q4_0.bin\n",
            "error loading model: failed to open ./models/7B/ggml-model-q4_0.bin.1: No such file or directory\n",
            "llama_init_from_file: failed to load model\n",
            "main: error: failed to load model './models/7B/ggml-model-q4_0.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!cmake --build . --config Release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmmbDPPZyqQS",
        "outputId": "8fddbf5b-389c-44c3-c3b0-112d6b77120a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/build\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found Threads: TRUE  \n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- x86 detected\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  5%] \u001b[32mBuilding C object CMakeFiles/ggml.dir/ggml.c.o\u001b[0m\n",
            "[  5%] Built target ggml\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/llama.cpp:67:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n",
            "   67 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/llama.cpp:79:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n",
            "   79 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K/content/llama.cpp/llama.cpp:92:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n",
            "   92 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "[ 16%] \u001b[32m\u001b[1mLinking CXX static library libllama.a\u001b[0m\n",
            "[ 16%] Built target llama\n",
            "[ 22%] \u001b[32mBuilding C object tests/CMakeFiles/test-quantize.dir/test-quantize.c.o\u001b[0m\n",
            "[ 27%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize\u001b[0m\n",
            "[ 27%] Built target test-quantize\n",
            "[ 33%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "\u001b[01m\u001b[K/content/llama.cpp/tests/test-tokenizer-0.cpp:19:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n",
            "   19 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 38%] Built target test-tokenizer-0\n",
            "[ 44%] \u001b[32mBuilding CXX object examples/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 44%] Built target common\n",
            "[ 50%] \u001b[32mBuilding CXX object examples/main/CMakeFiles/main.dir/main.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/main\u001b[0m\n",
            "[ 55%] Built target main\n",
            "[ 61%] \u001b[32mBuilding CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize\u001b[0m\n",
            "[ 66%] Built target quantize\n",
            "[ 72%] \u001b[32mBuilding CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/quantize-stats\u001b[0m\n",
            "[ 77%] Built target quantize-stats\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/perplexity\u001b[0m\n",
            "[ 88%] Built target perplexity\n",
            "[ 94%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/embedding\u001b[0m\n",
            "[100%] Built target embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6IRnXGx0bZe",
        "outputId": "a5c7acba-32c1-44bc-c9a9-11832dfe4a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build\t\t\t flake.lock  llama_util.h   quantize-stats\n",
            "build.zig\t\t flake.nix   main\t    README.md\n",
            "CMakeLists.txt\t\t ggml.c      Makefile\t    requirements.txt\n",
            "common.o\t\t ggml.h      media\t    SHA256SUMS\n",
            "convert-lora-to-ggml.py  ggml.o      models\t    spm-headers\n",
            "convert-pth-to-ggml.py\t LICENSE     Package.swift  tests\n",
            "convert.py\t\t llama.cpp   perplexity\n",
            "embedding\t\t llama.h     prompts\n",
            "examples\t\t llama.o     quantize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!../examples/alpaca.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcaKfgw70ZAz",
        "outputId": "eeb9fe15-d1cb-43f7-c146-ead0c67a8900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main: seed = 1681795454\n",
            "error loading model: failed to open ./models/ggml-alpaca-7b-q4.bin: No such file or directory\n",
            "llama_init_from_file: failed to load model\n",
            "main: error: failed to load model './models/ggml-alpaca-7b-q4.bin'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4G5GfaXyu6B",
        "outputId": "e85483f2-4f13-4312-eadd-bbb6a5ae8cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './models': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!65B 30B 13B 7B tokenizer_checklist.chk tokenizer.model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDTAjS8szAB0",
        "outputId": "94c75bb6-01f7-42b1-d713-ec18c000ed32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: 65B: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS1tAHigzCgp",
        "outputId": "5a269197-a90a-47dc-c389-7cec4ea81116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.24\n",
            "  Downloading numpy-1.24.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.98\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece, numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.0 which is incompatible.\n",
            "seaborn 0.12.2 requires numpy!=1.24.0,>=1.17, but you have numpy 1.24.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.0 sentencepiece-0.1.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 convert.py models/7B/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH4TyURDzJvh",
        "outputId": "d84dced7-d109-4522-f090-8143ed257ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model file models/7B\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/llama.cpp/convert.py\", line 1149, in <module>\n",
            "    main()\n",
            "  File \"/content/llama.cpp/convert.py\", line 1129, in main\n",
            "    model_plus = load_some_model(args.model)\n",
            "  File \"/content/llama.cpp/convert.py\", line 1055, in load_some_model\n",
            "    models_plus.append(lazy_load_file(path))\n",
            "  File \"/content/llama.cpp/convert.py\", line 844, in lazy_load_file\n",
            "    fp = open(path, 'rb')\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'models/7B'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_sf-idQVzQkY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}